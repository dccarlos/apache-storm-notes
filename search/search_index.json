{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Storm Notes","title":"Home"},{"location":"#apache-storm-notes","text":"","title":"Apache Storm Notes"},{"location":"notes/resources/","text":"Links Storm in pictures Mastering Apache Storm","title":"Resources"},{"location":"notes/resources/#links","text":"Storm in pictures Mastering Apache Storm","title":"Links"},{"location":"notes/module-01/01-introduction/","text":"Introduction What's Apache Storm? Apache Storm is a free and open source distributed realtime computation system . It makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing . Storm is to real-time processing what Hadoop is to batch processing. Use cases Stream processing Continuous computation (i.e.: aggregations) Distributed RPC (DRPC) The idea behind distributed RPC (DRPC) is to parallelize the computation of really intense functions on the fly using Storm Real-time analytics Features Fast Horizontally scalable Fault tolerant Guaranteed data processing Easy to operate Programming language agnostic Storm components Zookeeper A Storm cluster follows a master-slave model where the master and slave processes are coordinated through ZooKeeper . The following are the components of a Storm cluster. All of the states associated with the cluster and the various tasks submitted to Storm are stored in ZooKeeper. Nimbus and supervisor nodes do not communicate directly with each other, but through ZooKeeper. As all data is stored in ZooKeeper, both Nimbus and the supervisor daemons can be killed abruptly without adversely affecting the cluster. Nimbus The Nimbus node is the single stateless master in a Storm cluster (it stores all of its data in ZK). It is responsible for distributing the application code across various worker nodes, assigning tasks to different machines, monitoring tasks for any failures, and restarting them as and when required. If the active node goes down, then the passive node will become an Active node. The Storm workers can work smoothly even if all the Nimbus nodes go down but the user can't submit any new jobs into the cluster or the cluster will not be able to reassign the failed workers to another node. Supervisor nodes Supervisor nodes are the worker nodes in a Storm cluster . Each supervisor node runs a supervisor daemon that is responsible for creating, starting, and stopping worker processes to execute the tasks assigned to that node . Like Nimbus, a supervisor daemon is also fail- fast and stores all of its states in ZooKeeper so that it can be restarted without any state loss. A single supervisor daemon normally handles multiple worker processes running on that machine. Storm data model The basic unit of data that can be processed by a Storm application is called a tuple . Each tuple consists of a predefined list of fields. The value of each field can be a byte, char, integer, long, float, double, Boolean, or byte array. Storm also provides an API to define your own datatypes, which can be serialized as fields in a tuple. A tuple is dynamically typed, that is, you just need to define the names of the fields in a tuple and not their datatype. Storm topology In Storm terminology, a topology is an abstraction that defines the graph of the computation. You create a Storm topology and deploy it on a Storm cluster to process data . A topology can be represented by a direct acyclic graph (DAG), where each node does some kind of processing and forwards it to the next node(s) in the flow. The following diagram is a sample Storm topology: Tuple (Message) A single message/record that flows between the different instances of a topology is called a tuple. Stream The key abstraction in Storm is that of a stream. A stream is an unbounded sequence of tuples that can be processed in parallel by Storm. Each stream can be processed by a single or multiple types of bolts Spout (Source) A spout is the source of tuples in a Storm topology. It is responsible for reading or listening to data from an external source , for example, by reading from a log file or listening for new messages in a queue and publishing them\u2014 emitting into Storm streams . A spout can emit multiple streams, each of a different schema . Storm track all the tuples containing a message ID during the processing across the cluster, if tuples doesn't have a message ID tracking is skipped . Bolt (Processor) A bolt is the processing powerhouse of a Storm topology and is responsible for transforming a stream. Operation Modes Storm has two modes of operation: local mode and remote mode . In local mode, you can develop and test topologies completely in process on your local machine. In remote mode, you submit topologies for execution on a cluster of machines. TL;DR Taken from https://jansipke.nl/storm-in-pictures/ A topology is a description of a workflow. It defines how the code you write is put together and executed. A spout is responsible for data input. Storm does not impose any restriction on the source of the data. Hadoop, for example, wants the data to be in its own filesystem, HDFS. With Storm, you can use any data source you want as long as you\u2019re able to write a piece of code for it that fetches this data. Typically, the input data comes from a queue such as Kafka or ActiveMQ, but databases, filesystems or web servers are also fine. A bolt is responsible for processing the data. It gets data from a spout or another bolt in the form of tuples. A tuple is a lightweight data format provided by Storm that you can use to wrap the data you actually need to process. A bolt either processes the input data and sends it out as another tuple (or set of tuples), or it stores it in some external system. This could be yet another queue, database, filesystem, etc. Storm 0.9 added the notion of a metrics consumer. It is often helpful to know what is going on in your topology. The Storm UI provides some insight, but this is on a general level, i.e. it shows how many tuples were transmitted between spouts and bolts, and how many were acknowledged or failed. It can\u2019t tell anything about the internal state of your spouts and bolts, because that is application specific. This is where the metrics framework, and the metrics consumer in particular, comes into play. The metrics framework allows you to create metrics variables in your spouts and bolts. These metrics are transmitted to the metrics consumer. Like the bolts, it is then your responsibility to push these metrics to an external system for storage or visualization. There is a single master node that is responsible for the scheduling of tasks in the cluster. One process on this machine is called Nimbus, which performs the actual scheduling of tasks. Another process is the Storm UI, which can be used to view the cluster and the topologies. There are several slave nodes that actually execute your code. One process on these machines is the Supervisor, which supervises the process that actually executes your code. Another process, new since Storm 0.9, is the Logviewer. It is now possible to use the Storm UI, pinpoint any problem in the execution of your code and then click through to the logfile on the slave node that executed the code. Storm uses ZooKeeper to perform cluster management. In a development environment, a single ZooKeeper node is fine. In a production environment, it is necessary to use three, five or more ( 2n+1) nodes. Storm tries to make use of as much parallelism as possible. To achieve this, it uses multiple machines (supervisors), runs several Java virtual machines on each machine (workers) and uses many threads per JVM (executors). The following picture illustrates this: The number of supervisors depends on the number of machines you have installed the Storm supervisor process on. The number of workers each of these machines run is configured in the * storm.yaml* configuration file. By default there are four worker processes (JVMs) per supervisor, but this can be changed by adding port numbers to this file. supervisor.slots.ports : - 6700 - 6701 - 6702 - 6703 The number of supervisors and workers that are useable by Storm are now set. Now we can tell Storm how we want to run our topology on this cluster: We start by creating a Config object and setting the number of workers we want to use for this specific topology. In this case we selected two of them. Then we create a TopologyBuilder object and set our spout, our bolts and our metrics consumer. We can set two parameters that tell Storm how many to have of these components and how parallel they should be executed: The first parameter is the parallelism hint. This tells Storm how many executors (threads) should be used for this component. By default this number is equal to 1. The second number we can set is the number of tasks. This tells Storm how many times the component should be present in total. If the number of tasks is higher than the parallelism hint, then there will be executors that run more than one task serially. For example, when using a parallelism hint of 2 and a number of tasks of 6, there will be 2 executors that run 3 components serially. By default the number of tasks is equal to the parallelism hint.","title":"1. Introduction"},{"location":"notes/module-01/01-introduction/#introduction","text":"","title":"Introduction"},{"location":"notes/module-01/01-introduction/#whats-apache-storm","text":"Apache Storm is a free and open source distributed realtime computation system . It makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing . Storm is to real-time processing what Hadoop is to batch processing.","title":"What's Apache Storm?"},{"location":"notes/module-01/01-introduction/#use-cases","text":"Stream processing Continuous computation (i.e.: aggregations) Distributed RPC (DRPC) The idea behind distributed RPC (DRPC) is to parallelize the computation of really intense functions on the fly using Storm Real-time analytics","title":"Use cases"},{"location":"notes/module-01/01-introduction/#features","text":"Fast Horizontally scalable Fault tolerant Guaranteed data processing Easy to operate Programming language agnostic","title":"Features"},{"location":"notes/module-01/01-introduction/#storm-components","text":"","title":"Storm components"},{"location":"notes/module-01/01-introduction/#zookeeper","text":"A Storm cluster follows a master-slave model where the master and slave processes are coordinated through ZooKeeper . The following are the components of a Storm cluster. All of the states associated with the cluster and the various tasks submitted to Storm are stored in ZooKeeper. Nimbus and supervisor nodes do not communicate directly with each other, but through ZooKeeper. As all data is stored in ZooKeeper, both Nimbus and the supervisor daemons can be killed abruptly without adversely affecting the cluster.","title":"Zookeeper"},{"location":"notes/module-01/01-introduction/#nimbus","text":"The Nimbus node is the single stateless master in a Storm cluster (it stores all of its data in ZK). It is responsible for distributing the application code across various worker nodes, assigning tasks to different machines, monitoring tasks for any failures, and restarting them as and when required. If the active node goes down, then the passive node will become an Active node. The Storm workers can work smoothly even if all the Nimbus nodes go down but the user can't submit any new jobs into the cluster or the cluster will not be able to reassign the failed workers to another node.","title":"Nimbus"},{"location":"notes/module-01/01-introduction/#supervisor-nodes","text":"Supervisor nodes are the worker nodes in a Storm cluster . Each supervisor node runs a supervisor daemon that is responsible for creating, starting, and stopping worker processes to execute the tasks assigned to that node . Like Nimbus, a supervisor daemon is also fail- fast and stores all of its states in ZooKeeper so that it can be restarted without any state loss. A single supervisor daemon normally handles multiple worker processes running on that machine.","title":"Supervisor nodes"},{"location":"notes/module-01/01-introduction/#storm-data-model","text":"The basic unit of data that can be processed by a Storm application is called a tuple . Each tuple consists of a predefined list of fields. The value of each field can be a byte, char, integer, long, float, double, Boolean, or byte array. Storm also provides an API to define your own datatypes, which can be serialized as fields in a tuple. A tuple is dynamically typed, that is, you just need to define the names of the fields in a tuple and not their datatype.","title":"Storm data model"},{"location":"notes/module-01/01-introduction/#storm-topology","text":"In Storm terminology, a topology is an abstraction that defines the graph of the computation. You create a Storm topology and deploy it on a Storm cluster to process data . A topology can be represented by a direct acyclic graph (DAG), where each node does some kind of processing and forwards it to the next node(s) in the flow. The following diagram is a sample Storm topology:","title":"Storm topology"},{"location":"notes/module-01/01-introduction/#tuple-message","text":"A single message/record that flows between the different instances of a topology is called a tuple.","title":"Tuple (Message)"},{"location":"notes/module-01/01-introduction/#stream","text":"The key abstraction in Storm is that of a stream. A stream is an unbounded sequence of tuples that can be processed in parallel by Storm. Each stream can be processed by a single or multiple types of bolts","title":"Stream"},{"location":"notes/module-01/01-introduction/#spout-source","text":"A spout is the source of tuples in a Storm topology. It is responsible for reading or listening to data from an external source , for example, by reading from a log file or listening for new messages in a queue and publishing them\u2014 emitting into Storm streams . A spout can emit multiple streams, each of a different schema . Storm track all the tuples containing a message ID during the processing across the cluster, if tuples doesn't have a message ID tracking is skipped .","title":"Spout (Source)"},{"location":"notes/module-01/01-introduction/#bolt-processor","text":"A bolt is the processing powerhouse of a Storm topology and is responsible for transforming a stream.","title":"Bolt (Processor)"},{"location":"notes/module-01/01-introduction/#operation-modes","text":"Storm has two modes of operation: local mode and remote mode . In local mode, you can develop and test topologies completely in process on your local machine. In remote mode, you submit topologies for execution on a cluster of machines.","title":"Operation Modes"},{"location":"notes/module-01/01-introduction/#tldr","text":"Taken from https://jansipke.nl/storm-in-pictures/ A topology is a description of a workflow. It defines how the code you write is put together and executed. A spout is responsible for data input. Storm does not impose any restriction on the source of the data. Hadoop, for example, wants the data to be in its own filesystem, HDFS. With Storm, you can use any data source you want as long as you\u2019re able to write a piece of code for it that fetches this data. Typically, the input data comes from a queue such as Kafka or ActiveMQ, but databases, filesystems or web servers are also fine. A bolt is responsible for processing the data. It gets data from a spout or another bolt in the form of tuples. A tuple is a lightweight data format provided by Storm that you can use to wrap the data you actually need to process. A bolt either processes the input data and sends it out as another tuple (or set of tuples), or it stores it in some external system. This could be yet another queue, database, filesystem, etc. Storm 0.9 added the notion of a metrics consumer. It is often helpful to know what is going on in your topology. The Storm UI provides some insight, but this is on a general level, i.e. it shows how many tuples were transmitted between spouts and bolts, and how many were acknowledged or failed. It can\u2019t tell anything about the internal state of your spouts and bolts, because that is application specific. This is where the metrics framework, and the metrics consumer in particular, comes into play. The metrics framework allows you to create metrics variables in your spouts and bolts. These metrics are transmitted to the metrics consumer. Like the bolts, it is then your responsibility to push these metrics to an external system for storage or visualization. There is a single master node that is responsible for the scheduling of tasks in the cluster. One process on this machine is called Nimbus, which performs the actual scheduling of tasks. Another process is the Storm UI, which can be used to view the cluster and the topologies. There are several slave nodes that actually execute your code. One process on these machines is the Supervisor, which supervises the process that actually executes your code. Another process, new since Storm 0.9, is the Logviewer. It is now possible to use the Storm UI, pinpoint any problem in the execution of your code and then click through to the logfile on the slave node that executed the code. Storm uses ZooKeeper to perform cluster management. In a development environment, a single ZooKeeper node is fine. In a production environment, it is necessary to use three, five or more ( 2n+1) nodes. Storm tries to make use of as much parallelism as possible. To achieve this, it uses multiple machines (supervisors), runs several Java virtual machines on each machine (workers) and uses many threads per JVM (executors). The following picture illustrates this: The number of supervisors depends on the number of machines you have installed the Storm supervisor process on. The number of workers each of these machines run is configured in the * storm.yaml* configuration file. By default there are four worker processes (JVMs) per supervisor, but this can be changed by adding port numbers to this file. supervisor.slots.ports : - 6700 - 6701 - 6702 - 6703 The number of supervisors and workers that are useable by Storm are now set. Now we can tell Storm how we want to run our topology on this cluster: We start by creating a Config object and setting the number of workers we want to use for this specific topology. In this case we selected two of them. Then we create a TopologyBuilder object and set our spout, our bolts and our metrics consumer. We can set two parameters that tell Storm how many to have of these components and how parallel they should be executed: The first parameter is the parallelism hint. This tells Storm how many executors (threads) should be used for this component. By default this number is equal to 1. The second number we can set is the number of tasks. This tells Storm how many times the component should be present in total. If the number of tasks is higher than the parallelism hint, then there will be executors that run more than one task serially. For example, when using a parallelism hint of 2 and a number of tasks of 6, there will be 2 executors that run 3 components serially. By default the number of tasks is equal to the parallelism hint.","title":"TL;DR"},{"location":"notes/module-01/02-topology-development/","text":"Deployment Zookeeper Being a distributed application, Storm also uses a ZooKeeper cluster to coordinate various processes. All of the states associated with the cluster and the various tasks submitted to Storm are stored in ZooKeeper. In the ZooKeeper ensemble, one node in the cluster acts as the leader, while the rest are followers. If the leader node of the ZooKeeper cluster dies, then an election for the new leader takes places among the remaining live nodes, and a new leader is elected. All write requests coming from clients are forwarded to the leader node, while the follower nodes only handle the read requests. Also, we can't increase the write performance of the ZooKeeper ensemble by increasing the number of nodes because all write operations go through the leader node. It is advised to run an odd number of ZooKeeper nodes, as the ZooKeeper cluster keeps working as long as the majority (the number of live nodes is greater than n/2 , where n being the number of deployed nodes) of the nodes are running. So if we have a cluster of four ZooKeeper nodes ( 3 > 4/2 ; only one node can die), then we can handle only one node failure, while if we had five nodes ( 3 > 5/2 ; two nodes can die) in the cluster, then we can handle two node failures. Hello world sample Spout (Source) sample This spout does not connect to an external source to fetch data, but randomly generates the data and emits a continuous stream of records. public class SampleSpout extends BaseRichSpout { private static final long serialVersionUID = 1L ; private static final Map < Integer , String > map = new HashMap < Integer , String > (); static { map . put ( 0 , \"google\" ); map . put ( 1 , \"facebook\" ); map . put ( 2 , \"twitter\" ); map . put ( 3 , \"youtube\" ); map . put ( 4 , \"linkedin\" ); } private SpoutOutputCollector spoutOutputCollector ; @SuppressWarnings ( \"rawtypes\" ) public void open ( Map conf , TopologyContext context , SpoutOutputCollector spoutOutputCollector ) { // Open the spout this . spoutOutputCollector = spoutOutputCollector ; } public void nextTuple () { // Storm cluster repeatedly calls this method to emit continuous // stream of tuples. final Random rand = new Random (); // generate the random number from 0 to 4. int randomNumber = rand . nextInt ( 5 ); spoutOutputCollector . emit ( new Values ( map . get ( randomNumber ))); try { Thread . sleep ( 5000 ); } catch ( Exception e ) { System . out . println ( \"Failed to sleep the thread\" ); } } public void declareOutputFields ( OutputFieldsDeclarer declarer ) { // emit the tuple with field \"site\" declarer . declare ( new Fields ( \"site\" )); } } Bold (Processor) sample This bolt will consume the tuples emitted by the SampleSpout spout and will print the value of the field site on the console. public class SampleBolt extends BaseBasicBolt { private static final long serialVersionUID = 1L ; public void execute ( Tuple input , BasicOutputCollector collector ) { // fetched the field \"site\" from input tuple. String test = input . getStringByField ( \"site\" ); // print the value of field \"site\" on console. System . out . println ( \"######### Name of input site is : \" + test ); } public void declareOutputFields ( OutputFieldsDeclarer declarer ) { } } Topology (DAG) Create a main SampleStormTopology class within the same package. This class creates an instance of the spout and bolt along with the classes, and chaines them together using a TopologyBuilder class. This class uses org.apache.storm.LocalCluster to simulate the Storm cluster. The LocalCluster mode is used for debugging/testing the topology on a developer machine before deploying it on the Storm cluster. The following is the implementation of the main class for running locally: Prepare local package com.stormadvance.storm_example ; import org.apache.storm.Config ; import org.apache.storm.LocalCluster ; import org.apache.storm.topology.TopologyBuilder ; public class SampleStormTopology { @SuppressWarnings ( \"resource\" ) public static void main ( String [] args ) throws Exception { // create an instance of TopologyBuilder class TopologyBuilder builder = new TopologyBuilder (); // set the spout class builder . setSpout ( \"SampleSpout\" , new SampleSpout (), 2 ); // set the bolt class builder . setBolt ( \"SampleBolt\" , new SampleBolt (), 4 ). shuffleGrouping ( \"SampleSpout\" ); Config conf = new Config (); conf . setDebug ( true ); // create an instance of LocalCluster class for // executing topology in local mode. LocalCluster cluster = new LocalCluster (); // SampleStormTopology is the name of submitted topology cluster . submitTopology ( \"SampleStormTopology\" , conf , builder . createTopology ()); try { Thread . sleep ( 100000 ); } catch ( Exception exception ) { System . out . println ( \"Thread interrupted exception : \" + exception ); } // kill the SampleStormTopology cluster . killTopology ( \"SampleStormTopology\" ); // shutdown the storm test cluster cluster . shutdown (); } } Prepare for cluster public class SampleStormClusterTopology { public static void main ( String [] args ) throws AlreadyAliveException , InvalidTopologyException { // create an instance of TopologyBuilder class TopologyBuilder builder = new TopologyBuilder (); // set the spout class builder . setSpout ( \"SampleSpout\" , new SampleSpout (), 2 ); // set the bolt class builder . setBolt ( \"SampleBolt\" , new SampleBolt (), 4 ). shuffleGrouping ( \"SampleSpout\" ); Config conf = new Config (); conf . setNumWorkers ( 3 ); // This statement submit the topology on remote // args[0] = name of topology try { StormSubmitter . submitTopology ( args [ 0 ] , conf , builder . createTopology ()); } catch ( AlreadyAliveException alreadyAliveException ) { System . out . println ( alreadyAliveException ); } catch ( InvalidTopologyException invalidTopologyException ) { System . out . println ( invalidTopologyException ); } catch ( AuthorizationException e ) { // TODO Auto-generated catch block e . printStackTrace (); } } } Execute in local # go to storm_example mvn clean install mvn compile exec:java -Dexec.classpathScope = compile -Dexec.mainClass = com.stormadvance.storm_example.SampleStormTopology Execute in cluster The structure of the command to execute in cluster (main node) is sh storm jar jarName.jar [TopologyMainClass] [Args] The main function of TopologyMainClass is to define the topology and submit it to the Nimbus machine. The storm jar part takes care of connecting to the Nimbus machine and uploading the JAR part. In storm nimbus (main node): mvn clean install # Move to main node docker cp target/storm_example-0.0.1-SNAPSHOT-jar-with-dependencies.jar s02-nimbus:/sample_topology.jar # Execute in container docker exec -it s02-nimbus storm jar /sample_topology.jar com.stormadvance.storm_example.SampleStormClusterTopology storm_example Storm cluster operations Deactivate Storm supports the deactivating a topology. In the deactivated state, spouts will not emit any new tuples into the pipeline, but the processing of the already emitted tuples will continue. Deactivating the topology does not free the Storm resource. If topology is activated, the spout again starts emitting tuples. sh storm deactivate topologyName Deactivating sample topology docker exec -it s02-nimbus storm deactivate storm_example Kill sh storm kill topologyName Storm topologies are never-ending processes . To stop a topology, we need to kill it. When killed, the topology first enters into the deactivation state, processes all the tuples already emitted into it, and then stops. Once it is killed, it will free all the Storm resources allotted to this topology. Killing sample topology docker exec -it s02-nimbus storm deactivate storm_example Rebalance Change log level settings storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT] The TIMEOUT is the time in seconds. The log level will go back to normal after the timeout time. The value of TIMEOUT is mandatory if you are setting the log level to DEBUG / ALL . Chaning log level settings docker exec -it s02-nimbus storm set_log_level storm_example -l ROOT = DEBUG:30 Storm UI While submitting a topology to the cluster, the user first needs to make sure that the value of the Free slots column should not be zero; otherwise, the topology doesn't get any worker for processing and will wait in the queue until a workers becomes free . A node with the status leader is an active master while the node with the status Not a Leader is a passive master .","title":"2. Topology and development"},{"location":"notes/module-01/02-topology-development/#deployment","text":"","title":"Deployment"},{"location":"notes/module-01/02-topology-development/#zookeeper","text":"Being a distributed application, Storm also uses a ZooKeeper cluster to coordinate various processes. All of the states associated with the cluster and the various tasks submitted to Storm are stored in ZooKeeper. In the ZooKeeper ensemble, one node in the cluster acts as the leader, while the rest are followers. If the leader node of the ZooKeeper cluster dies, then an election for the new leader takes places among the remaining live nodes, and a new leader is elected. All write requests coming from clients are forwarded to the leader node, while the follower nodes only handle the read requests. Also, we can't increase the write performance of the ZooKeeper ensemble by increasing the number of nodes because all write operations go through the leader node. It is advised to run an odd number of ZooKeeper nodes, as the ZooKeeper cluster keeps working as long as the majority (the number of live nodes is greater than n/2 , where n being the number of deployed nodes) of the nodes are running. So if we have a cluster of four ZooKeeper nodes ( 3 > 4/2 ; only one node can die), then we can handle only one node failure, while if we had five nodes ( 3 > 5/2 ; two nodes can die) in the cluster, then we can handle two node failures.","title":"Zookeeper"},{"location":"notes/module-01/02-topology-development/#hello-world-sample","text":"","title":"Hello world sample"},{"location":"notes/module-01/02-topology-development/#spout-source-sample","text":"This spout does not connect to an external source to fetch data, but randomly generates the data and emits a continuous stream of records. public class SampleSpout extends BaseRichSpout { private static final long serialVersionUID = 1L ; private static final Map < Integer , String > map = new HashMap < Integer , String > (); static { map . put ( 0 , \"google\" ); map . put ( 1 , \"facebook\" ); map . put ( 2 , \"twitter\" ); map . put ( 3 , \"youtube\" ); map . put ( 4 , \"linkedin\" ); } private SpoutOutputCollector spoutOutputCollector ; @SuppressWarnings ( \"rawtypes\" ) public void open ( Map conf , TopologyContext context , SpoutOutputCollector spoutOutputCollector ) { // Open the spout this . spoutOutputCollector = spoutOutputCollector ; } public void nextTuple () { // Storm cluster repeatedly calls this method to emit continuous // stream of tuples. final Random rand = new Random (); // generate the random number from 0 to 4. int randomNumber = rand . nextInt ( 5 ); spoutOutputCollector . emit ( new Values ( map . get ( randomNumber ))); try { Thread . sleep ( 5000 ); } catch ( Exception e ) { System . out . println ( \"Failed to sleep the thread\" ); } } public void declareOutputFields ( OutputFieldsDeclarer declarer ) { // emit the tuple with field \"site\" declarer . declare ( new Fields ( \"site\" )); } }","title":"Spout (Source) sample"},{"location":"notes/module-01/02-topology-development/#bold-processor-sample","text":"This bolt will consume the tuples emitted by the SampleSpout spout and will print the value of the field site on the console. public class SampleBolt extends BaseBasicBolt { private static final long serialVersionUID = 1L ; public void execute ( Tuple input , BasicOutputCollector collector ) { // fetched the field \"site\" from input tuple. String test = input . getStringByField ( \"site\" ); // print the value of field \"site\" on console. System . out . println ( \"######### Name of input site is : \" + test ); } public void declareOutputFields ( OutputFieldsDeclarer declarer ) { } }","title":"Bold (Processor) sample"},{"location":"notes/module-01/02-topology-development/#topology-dag","text":"Create a main SampleStormTopology class within the same package. This class creates an instance of the spout and bolt along with the classes, and chaines them together using a TopologyBuilder class. This class uses org.apache.storm.LocalCluster to simulate the Storm cluster. The LocalCluster mode is used for debugging/testing the topology on a developer machine before deploying it on the Storm cluster. The following is the implementation of the main class for running locally:","title":"Topology (DAG)"},{"location":"notes/module-01/02-topology-development/#prepare-local","text":"package com.stormadvance.storm_example ; import org.apache.storm.Config ; import org.apache.storm.LocalCluster ; import org.apache.storm.topology.TopologyBuilder ; public class SampleStormTopology { @SuppressWarnings ( \"resource\" ) public static void main ( String [] args ) throws Exception { // create an instance of TopologyBuilder class TopologyBuilder builder = new TopologyBuilder (); // set the spout class builder . setSpout ( \"SampleSpout\" , new SampleSpout (), 2 ); // set the bolt class builder . setBolt ( \"SampleBolt\" , new SampleBolt (), 4 ). shuffleGrouping ( \"SampleSpout\" ); Config conf = new Config (); conf . setDebug ( true ); // create an instance of LocalCluster class for // executing topology in local mode. LocalCluster cluster = new LocalCluster (); // SampleStormTopology is the name of submitted topology cluster . submitTopology ( \"SampleStormTopology\" , conf , builder . createTopology ()); try { Thread . sleep ( 100000 ); } catch ( Exception exception ) { System . out . println ( \"Thread interrupted exception : \" + exception ); } // kill the SampleStormTopology cluster . killTopology ( \"SampleStormTopology\" ); // shutdown the storm test cluster cluster . shutdown (); } }","title":"Prepare local"},{"location":"notes/module-01/02-topology-development/#prepare-for-cluster","text":"public class SampleStormClusterTopology { public static void main ( String [] args ) throws AlreadyAliveException , InvalidTopologyException { // create an instance of TopologyBuilder class TopologyBuilder builder = new TopologyBuilder (); // set the spout class builder . setSpout ( \"SampleSpout\" , new SampleSpout (), 2 ); // set the bolt class builder . setBolt ( \"SampleBolt\" , new SampleBolt (), 4 ). shuffleGrouping ( \"SampleSpout\" ); Config conf = new Config (); conf . setNumWorkers ( 3 ); // This statement submit the topology on remote // args[0] = name of topology try { StormSubmitter . submitTopology ( args [ 0 ] , conf , builder . createTopology ()); } catch ( AlreadyAliveException alreadyAliveException ) { System . out . println ( alreadyAliveException ); } catch ( InvalidTopologyException invalidTopologyException ) { System . out . println ( invalidTopologyException ); } catch ( AuthorizationException e ) { // TODO Auto-generated catch block e . printStackTrace (); } } }","title":"Prepare for cluster"},{"location":"notes/module-01/02-topology-development/#execute-in-local","text":"# go to storm_example mvn clean install mvn compile exec:java -Dexec.classpathScope = compile -Dexec.mainClass = com.stormadvance.storm_example.SampleStormTopology","title":"Execute in local"},{"location":"notes/module-01/02-topology-development/#execute-in-cluster","text":"The structure of the command to execute in cluster (main node) is sh storm jar jarName.jar [TopologyMainClass] [Args] The main function of TopologyMainClass is to define the topology and submit it to the Nimbus machine. The storm jar part takes care of connecting to the Nimbus machine and uploading the JAR part. In storm nimbus (main node): mvn clean install # Move to main node docker cp target/storm_example-0.0.1-SNAPSHOT-jar-with-dependencies.jar s02-nimbus:/sample_topology.jar # Execute in container docker exec -it s02-nimbus storm jar /sample_topology.jar com.stormadvance.storm_example.SampleStormClusterTopology storm_example","title":"Execute in cluster"},{"location":"notes/module-01/02-topology-development/#storm-cluster-operations","text":"","title":"Storm cluster operations"},{"location":"notes/module-01/02-topology-development/#deactivate","text":"Storm supports the deactivating a topology. In the deactivated state, spouts will not emit any new tuples into the pipeline, but the processing of the already emitted tuples will continue. Deactivating the topology does not free the Storm resource. If topology is activated, the spout again starts emitting tuples. sh storm deactivate topologyName","title":"Deactivate"},{"location":"notes/module-01/02-topology-development/#deactivating-sample-topology","text":"docker exec -it s02-nimbus storm deactivate storm_example","title":"Deactivating sample topology"},{"location":"notes/module-01/02-topology-development/#kill","text":"sh storm kill topologyName Storm topologies are never-ending processes . To stop a topology, we need to kill it. When killed, the topology first enters into the deactivation state, processes all the tuples already emitted into it, and then stops. Once it is killed, it will free all the Storm resources allotted to this topology.","title":"Kill"},{"location":"notes/module-01/02-topology-development/#killing-sample-topology","text":"docker exec -it s02-nimbus storm deactivate storm_example","title":"Killing sample topology"},{"location":"notes/module-01/02-topology-development/#rebalance","text":"","title":"Rebalance"},{"location":"notes/module-01/02-topology-development/#change-log-level-settings","text":"storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT] The TIMEOUT is the time in seconds. The log level will go back to normal after the timeout time. The value of TIMEOUT is mandatory if you are setting the log level to DEBUG / ALL .","title":"Change log level settings"},{"location":"notes/module-01/02-topology-development/#chaning-log-level-settings","text":"docker exec -it s02-nimbus storm set_log_level storm_example -l ROOT = DEBUG:30","title":"Chaning log level settings"},{"location":"notes/module-01/02-topology-development/#storm-ui","text":"While submitting a topology to the cluster, the user first needs to make sure that the value of the Free slots column should not be zero; otherwise, the topology doesn't get any worker for processing and will wait in the queue until a workers becomes free . A node with the status leader is an active master while the node with the status Not a Leader is a passive master .","title":"Storm UI"},{"location":"notes/module-01/03-parallelism-and-partitioning/","text":"Storm Parallelism and Data Partitioning Parallelism of a topology Parallelism means the distribution of jobs on multiple nodes/instances where each instance can work independently and can contribute to the processing of data. Worker process (A JVM instance) A Storm topology is executed across multiple supervisor nodes in the Storm cluster. Each of the nodes in the cluster can run one or more JVMs called worker processes , which are responsible for processing a part of the topology. A worker process is specific to one of the specific topologies and can execute multiple components of that topology. If multiple topologies are being run at the same time, none of them will share any of the workers , thus providing some degree of isolation between topologies. Executor Within each worker process, there can be multiple threads executing parts of the topology. Each of these threads is called an executor. An executor can execute only one of the components, that is, any spout or bolt in the topology . Each executor, being a single thread, can execute only tasks assigned to it serially. The number of executors defined for a spout or bolt can be changed dynamically while the topology is running. Task This is the most granular unit of task execution in Storm. Each task is an instance of a spout or bolt. When defining a Storm topology, you can specify the number of tasks for each spout and bolt. Once defined, the number of tasks cannot be changed for a component at runtime. Each task can be executed alone or with another task of the same type, or another instance of the same spout or bolt. Configure parallelism at the code level Storm provides an API to set the number of worker processes, number of executors, and number of tasks at the code level. Config conf = new Config (); conf . setNumWorkers ( 3 ); We can set the number of executors at the code level by passing the parallelism_hint argument in the setSpout and setBolt methods of class org.apache.storm.topology.TopologyBuilder . In code sample, we set parallelism_hint=2 for SampleSpout and parallelism_hint=4 for SampleBolt . At the time of execution, Storm will assign two executors for SampleSpout and four executors for SampleBolt . We can configure the number of tasks that can execute inside the executors. Here is the code snippet to show these settings in practice: builder . setSpout ( \"SampleSpout\" , new SampleSpout (), 2 ). setNumTasks ( 4 ); In the preceding code, we have configured the two executors and four tasks of SampleSpout . For SampleSpout , Storm will assign two tasks per executor. By default, Storm will run one task per executor if the user does not set the number of tasks at the code level . Worker process, executor, and task distribution Let's assume that we have: Worker processes set for the topology: 3 Executors per SampleSpout : 3 Executors per SampleBolt : 3 Tasks of SampleBolt : 6 Since SampleBolt are 6 tasks, it means 2 tasks per each of its 3 executors So we have that: total parallelism = 3 spouts + 6 bolts = 9 tasks Rebalance the parallelism of a topology One of the key features of Storm is that it allows us to modify the parallelism of a topology at runtime. The process of updating a topology parallelism at runtime is called rebalance. There are two ways to rebalance the topology: Using Storm Web UI Using Storm CLI Here are the commands that we need to execute on Storm CLI to rebalance the topology: storm rebalance [ TopologyName ] -n [ NumberOfWorkers ] \\ -e [ Spout ]=[ NumberOfExecutos ] \\ -e [ Bolt1 ]=[ NumberOfExecutos ] [ Bolt2 ]=[ NumberOfExecutos ] The rebalance command will first deactivate the topology for the duration of the message timeout and then redistribute the workers evenly around the Storm cluster. After a few seconds or minutes, the topology will revert to the previous state of activation and restart the processing of input streams. Different types of stream grouping (partitioning) in the Storm cluster Stream grouping in Storm provides complete control over how partitioning of tuples happens among the many tasks of a bolt subscribed to a stream. Storm supports the following types of stream groupings (partitioners): Suffle grouping Shuffle grouping distributes tuples in a uniform, random way across the tasks. An equal number of tuples will be processed by each task. This grouping is ideal when you want to distribute your processing load uniformly across the tasks and where there is no requirement for any data-driven partitioning. Field grouping As a result of the field grouping being hash (fields) % (no. of tasks) , it does not guarantee that each of the tasks will get tuples to process. For example, if you have applied a field grouping on a field, say X , with only two possible values, A and B , and created two tasks for the bolt, then it might be possible that both hash (A) % 2 and hash (B) % 2 return equal values, which will result in all the tuples being routed to a single task and the other being completely idle. Another common usage of field grouping is to join streams. Since partitioning happens solely on the basis of field values, and not the stream type, we can join two streams with any common join fields. The name of the fields needs not be the same. For example, in the order processing domain, we can join the Order stream and the ItemScanned stream to see when an order is completed: builder . setSpout ( \"1\" , new OrderSpout ()); builder . setSpout ( \"2\" , new ItemScannedSpout ()); builder . setBolt ( \"joiner\" , new OrderJoiner ()) . fieldsGrouping ( \"1\" , new Fields ( \"orderId\" )) . fieldsGrouping ( \"2\" , new Fields ( \"orderRefId\" )); All grouping All grouping is a special grouping that does not partition the tuples but replicates them to all the tasks, that is, each tuple will be sent to each of the bolt's tasks for processing . One common use case of all grouping is for sending signals to bolts. For example, if you are doing some kind of filtering on the streams, you can pass or change the filter parameters to all the bolts by sending them those parameters over a stream that is subscribed by all the bolt's tasks with an all grouping. Another example is to send a reset message to all the tasks in an aggregation bolt. Global grouping Global grouping does not partition the stream but sends the complete stream to the bolt's task, the smallest ID. Global grouping might seem redundant at first, as you can achieve the same results by defining the parallelism for the bolt as one if you only have one input stream. However, when you have multiple streams of data coming through a different path, you might want only one of the streams to be reduced and others to be parallel processes. For example, consider the following topology. In this, you might want to combine all the tuples coming from Bolt C in a single Bolt D task, while you might still want parallelism for tuples coming from Bolt E to Bolt D: Direct grouping In direct grouping, the emitter decides where each tuple will go for processing. For example, say we have a log stream and we want to process each log entry to be processed by a specific bolt task on the basis of the type of resource. In this case, we can use direct grouping. Direct grouping can only be used with direct streams. The emitDirect method takes a taskId parameter to specify the task. You can get the number of tasks for a component using the backtype.storm.task.TopologyContext.getComponentTasks method. Local or shuffle grouping If the tuple source and target bolt tasks are running in the same worker, using this grouping will act as a shuffle grouping only between the target tasks running on the same worker, thus minimizing any network hops, resulting in increased performance. If there are no target bolt tasks running on the source worker process, this grouping will act similar to the shuffle grouping mentioned earlier. None grouping None grouping is used when you don't care about the way tuples are partitioned among various tasks. As of Storm 0.8, this is equivalent to using shuffle grouping. Custom grouping If none of the preceding groupings fit your use case, you can define your own custom grouping by implementing the backtype.storm.grouping.CustomStreamGrouping interface. Guava is included in storm-core public class CategoryGrouping implements CustomStreamGrouping , Serializable { private static final Map < String , Integer > categories = ImmutableMap . of ( \"Financial\" , 0 , \"Medical\" , 1 , \"FMCG\" , 2 , \"Electronics\" , 3 ); private int tasks = 0 ; public void prepare ( WorkerTopologyContext context , GlobalStreamId stream , List < Integer > targetTasks ) { tasks = targetTasks . size (); } public List < Integer > chooseTasks ( int taskId , List < Object > values ) { String category = ( String ) values . get ( 0 ); return ImmutableList . of ( categories . get ( category ) % tasks ); } } The following diagram represents the Storm groupings graphically: Guaranteed message processing Storm provides two types of guarantees when processing tuples for a Storm topology. At least once Reliable; Tuples are processed at least once, but may be processed more than once. Use when subsecond latency is required and for unordered idempotent operations. Exactly once Tuples are processed only once. (This feature requires the use of a Trident spout and the Trident API. For more information) At-least-one Message Processing In a Storm topology, a single tuple being emitted by a spout can result in a number of tuples being generated in the later stages of the topology. Here, Spout A emits a tuple T(A) , which is processed by bolt B and bolt C, which emit tuple T(AB) and T(AC) respectively. So, when all the tuples produced as a result of tuple T(A) \u2014namely, the tuple tree T(A) , T(AB) , and T(AC) \u2014are processed, we say that the tuple has been processed completely. When some of the tuples in a tuple tree fail to process either due to some runtime error or a timeout that is configurable for each topology, then Storm considers that to be a failed tuple . Here are the six steps that are required by Storm to guarantee message processing: Tag each tuple emitted by a spout with a unique message ID. Storm uses this message ID to track the state of the tuple tree generated by this tuple. If you use one of the emit methods that doesn't take a messageId argument, Storm will not track it for complete processing. When the message is processed completely, Storm will send an acknowledgement with the same messageId that was used while emitting the tuple. A generic pattern implemented by spouts is that they read a message from a messaging queue, say RabbitMQ, produce the tuple into the topology for further processing, and then dequeue the message once it receives the acknowledgement that the tuple has been processed completely. When one of the bolts in the topology needs to produce a new tuple in the course of processing a message, for example, bolt B in the preceding topology, then it should emit the new tuple anchored with the original tuple that it got from the spout. Whenever you are done with processing a tuple in the execute method of your bolt, send an acknowledgment. If there is some problem in processing a tuple, a failure signal should be sent back. One of the general patterns of processing in Storm bolts is to process a tuple in, emit new tuples, and send an acknowledgement at the end of the execute method . This model results in at-least-once message processing semantics, and your application should be ready to handle a scenario when some of the messages will be processed multiple times . Tick tuple The tick tuple is a storm-generated tuple that we can configure at each bolt level, mainly for executing periodic tasks in the bolts (like cleaning a cache). The developer can configure the tick tuple at the code level while writing a bolt. public Map < String , Object > getComponentConfiguration () { Config conf = new Config (); int tickFrequencyInSeconds = 10 ; conf . put ( Config . TOPOLOGY_TICK_TUPLE_FREQ_SECS , tickFrequencyInSeconds ); return conf ; } In the preceding code, we have configured the tick tuple time to 10 seconds. Now, Storm will start generating a tick tuple after every 10 seconds. We'd require to prepare our code to identify it: public void execute ( Tuple tuple ) { if ( isTickTuple ( tuple )) { // now you can trigger e.g. a periodic activity } else { // do something with the normal tuple } } private static boolean isTickTuple ( Tuple tuple ) { return tuple . getSourceComponent (). equals ( Constants . SYSTEM_COMPONENT_ID ) && tuple . getSourceStreamId (). equals ( Constants . SYSTEM_TICK_STREAM_ID ); }","title":"3. Parallelism and partitioning"},{"location":"notes/module-01/03-parallelism-and-partitioning/#storm-parallelism-and-data-partitioning","text":"","title":"Storm Parallelism and Data Partitioning"},{"location":"notes/module-01/03-parallelism-and-partitioning/#parallelism-of-a-topology","text":"Parallelism means the distribution of jobs on multiple nodes/instances where each instance can work independently and can contribute to the processing of data.","title":"Parallelism of a topology"},{"location":"notes/module-01/03-parallelism-and-partitioning/#worker-process-a-jvm-instance","text":"A Storm topology is executed across multiple supervisor nodes in the Storm cluster. Each of the nodes in the cluster can run one or more JVMs called worker processes , which are responsible for processing a part of the topology. A worker process is specific to one of the specific topologies and can execute multiple components of that topology. If multiple topologies are being run at the same time, none of them will share any of the workers , thus providing some degree of isolation between topologies.","title":"Worker process (A JVM instance)"},{"location":"notes/module-01/03-parallelism-and-partitioning/#executor","text":"Within each worker process, there can be multiple threads executing parts of the topology. Each of these threads is called an executor. An executor can execute only one of the components, that is, any spout or bolt in the topology . Each executor, being a single thread, can execute only tasks assigned to it serially. The number of executors defined for a spout or bolt can be changed dynamically while the topology is running.","title":"Executor"},{"location":"notes/module-01/03-parallelism-and-partitioning/#task","text":"This is the most granular unit of task execution in Storm. Each task is an instance of a spout or bolt. When defining a Storm topology, you can specify the number of tasks for each spout and bolt. Once defined, the number of tasks cannot be changed for a component at runtime. Each task can be executed alone or with another task of the same type, or another instance of the same spout or bolt.","title":"Task"},{"location":"notes/module-01/03-parallelism-and-partitioning/#configure-parallelism-at-the-code-level","text":"Storm provides an API to set the number of worker processes, number of executors, and number of tasks at the code level. Config conf = new Config (); conf . setNumWorkers ( 3 ); We can set the number of executors at the code level by passing the parallelism_hint argument in the setSpout and setBolt methods of class org.apache.storm.topology.TopologyBuilder . In code sample, we set parallelism_hint=2 for SampleSpout and parallelism_hint=4 for SampleBolt . At the time of execution, Storm will assign two executors for SampleSpout and four executors for SampleBolt . We can configure the number of tasks that can execute inside the executors. Here is the code snippet to show these settings in practice: builder . setSpout ( \"SampleSpout\" , new SampleSpout (), 2 ). setNumTasks ( 4 ); In the preceding code, we have configured the two executors and four tasks of SampleSpout . For SampleSpout , Storm will assign two tasks per executor. By default, Storm will run one task per executor if the user does not set the number of tasks at the code level .","title":"Configure parallelism at the code level"},{"location":"notes/module-01/03-parallelism-and-partitioning/#worker-process-executor-and-task-distribution","text":"Let's assume that we have: Worker processes set for the topology: 3 Executors per SampleSpout : 3 Executors per SampleBolt : 3 Tasks of SampleBolt : 6 Since SampleBolt are 6 tasks, it means 2 tasks per each of its 3 executors So we have that: total parallelism = 3 spouts + 6 bolts = 9 tasks","title":"Worker process, executor, and task distribution"},{"location":"notes/module-01/03-parallelism-and-partitioning/#rebalance-the-parallelism-of-a-topology","text":"One of the key features of Storm is that it allows us to modify the parallelism of a topology at runtime. The process of updating a topology parallelism at runtime is called rebalance. There are two ways to rebalance the topology: Using Storm Web UI Using Storm CLI Here are the commands that we need to execute on Storm CLI to rebalance the topology: storm rebalance [ TopologyName ] -n [ NumberOfWorkers ] \\ -e [ Spout ]=[ NumberOfExecutos ] \\ -e [ Bolt1 ]=[ NumberOfExecutos ] [ Bolt2 ]=[ NumberOfExecutos ] The rebalance command will first deactivate the topology for the duration of the message timeout and then redistribute the workers evenly around the Storm cluster. After a few seconds or minutes, the topology will revert to the previous state of activation and restart the processing of input streams.","title":"Rebalance the parallelism of a topology"},{"location":"notes/module-01/03-parallelism-and-partitioning/#different-types-of-stream-grouping-partitioning-in-the-storm-cluster","text":"Stream grouping in Storm provides complete control over how partitioning of tuples happens among the many tasks of a bolt subscribed to a stream. Storm supports the following types of stream groupings (partitioners):","title":"Different types of stream grouping (partitioning) in the Storm cluster"},{"location":"notes/module-01/03-parallelism-and-partitioning/#suffle-grouping","text":"Shuffle grouping distributes tuples in a uniform, random way across the tasks. An equal number of tuples will be processed by each task. This grouping is ideal when you want to distribute your processing load uniformly across the tasks and where there is no requirement for any data-driven partitioning.","title":"Suffle grouping"},{"location":"notes/module-01/03-parallelism-and-partitioning/#field-grouping","text":"As a result of the field grouping being hash (fields) % (no. of tasks) , it does not guarantee that each of the tasks will get tuples to process. For example, if you have applied a field grouping on a field, say X , with only two possible values, A and B , and created two tasks for the bolt, then it might be possible that both hash (A) % 2 and hash (B) % 2 return equal values, which will result in all the tuples being routed to a single task and the other being completely idle. Another common usage of field grouping is to join streams. Since partitioning happens solely on the basis of field values, and not the stream type, we can join two streams with any common join fields. The name of the fields needs not be the same. For example, in the order processing domain, we can join the Order stream and the ItemScanned stream to see when an order is completed: builder . setSpout ( \"1\" , new OrderSpout ()); builder . setSpout ( \"2\" , new ItemScannedSpout ()); builder . setBolt ( \"joiner\" , new OrderJoiner ()) . fieldsGrouping ( \"1\" , new Fields ( \"orderId\" )) . fieldsGrouping ( \"2\" , new Fields ( \"orderRefId\" ));","title":"Field grouping"},{"location":"notes/module-01/03-parallelism-and-partitioning/#all-grouping","text":"All grouping is a special grouping that does not partition the tuples but replicates them to all the tasks, that is, each tuple will be sent to each of the bolt's tasks for processing . One common use case of all grouping is for sending signals to bolts. For example, if you are doing some kind of filtering on the streams, you can pass or change the filter parameters to all the bolts by sending them those parameters over a stream that is subscribed by all the bolt's tasks with an all grouping. Another example is to send a reset message to all the tasks in an aggregation bolt.","title":"All grouping"},{"location":"notes/module-01/03-parallelism-and-partitioning/#global-grouping","text":"Global grouping does not partition the stream but sends the complete stream to the bolt's task, the smallest ID. Global grouping might seem redundant at first, as you can achieve the same results by defining the parallelism for the bolt as one if you only have one input stream. However, when you have multiple streams of data coming through a different path, you might want only one of the streams to be reduced and others to be parallel processes. For example, consider the following topology. In this, you might want to combine all the tuples coming from Bolt C in a single Bolt D task, while you might still want parallelism for tuples coming from Bolt E to Bolt D:","title":"Global grouping"},{"location":"notes/module-01/03-parallelism-and-partitioning/#direct-grouping","text":"In direct grouping, the emitter decides where each tuple will go for processing. For example, say we have a log stream and we want to process each log entry to be processed by a specific bolt task on the basis of the type of resource. In this case, we can use direct grouping. Direct grouping can only be used with direct streams. The emitDirect method takes a taskId parameter to specify the task. You can get the number of tasks for a component using the backtype.storm.task.TopologyContext.getComponentTasks method.","title":"Direct grouping"},{"location":"notes/module-01/03-parallelism-and-partitioning/#local-or-shuffle-grouping","text":"If the tuple source and target bolt tasks are running in the same worker, using this grouping will act as a shuffle grouping only between the target tasks running on the same worker, thus minimizing any network hops, resulting in increased performance. If there are no target bolt tasks running on the source worker process, this grouping will act similar to the shuffle grouping mentioned earlier.","title":"Local or shuffle grouping"},{"location":"notes/module-01/03-parallelism-and-partitioning/#none-grouping","text":"None grouping is used when you don't care about the way tuples are partitioned among various tasks. As of Storm 0.8, this is equivalent to using shuffle grouping.","title":"None grouping"},{"location":"notes/module-01/03-parallelism-and-partitioning/#custom-grouping","text":"If none of the preceding groupings fit your use case, you can define your own custom grouping by implementing the backtype.storm.grouping.CustomStreamGrouping interface. Guava is included in storm-core public class CategoryGrouping implements CustomStreamGrouping , Serializable { private static final Map < String , Integer > categories = ImmutableMap . of ( \"Financial\" , 0 , \"Medical\" , 1 , \"FMCG\" , 2 , \"Electronics\" , 3 ); private int tasks = 0 ; public void prepare ( WorkerTopologyContext context , GlobalStreamId stream , List < Integer > targetTasks ) { tasks = targetTasks . size (); } public List < Integer > chooseTasks ( int taskId , List < Object > values ) { String category = ( String ) values . get ( 0 ); return ImmutableList . of ( categories . get ( category ) % tasks ); } } The following diagram represents the Storm groupings graphically:","title":"Custom grouping"},{"location":"notes/module-01/03-parallelism-and-partitioning/#guaranteed-message-processing","text":"Storm provides two types of guarantees when processing tuples for a Storm topology.","title":"Guaranteed message processing"},{"location":"notes/module-01/03-parallelism-and-partitioning/#at-least-once","text":"Reliable; Tuples are processed at least once, but may be processed more than once. Use when subsecond latency is required and for unordered idempotent operations.","title":"At least once"},{"location":"notes/module-01/03-parallelism-and-partitioning/#exactly-once","text":"Tuples are processed only once. (This feature requires the use of a Trident spout and the Trident API. For more information)","title":"Exactly once"},{"location":"notes/module-01/03-parallelism-and-partitioning/#at-least-one-message-processing","text":"In a Storm topology, a single tuple being emitted by a spout can result in a number of tuples being generated in the later stages of the topology. Here, Spout A emits a tuple T(A) , which is processed by bolt B and bolt C, which emit tuple T(AB) and T(AC) respectively. So, when all the tuples produced as a result of tuple T(A) \u2014namely, the tuple tree T(A) , T(AB) , and T(AC) \u2014are processed, we say that the tuple has been processed completely. When some of the tuples in a tuple tree fail to process either due to some runtime error or a timeout that is configurable for each topology, then Storm considers that to be a failed tuple . Here are the six steps that are required by Storm to guarantee message processing: Tag each tuple emitted by a spout with a unique message ID. Storm uses this message ID to track the state of the tuple tree generated by this tuple. If you use one of the emit methods that doesn't take a messageId argument, Storm will not track it for complete processing. When the message is processed completely, Storm will send an acknowledgement with the same messageId that was used while emitting the tuple. A generic pattern implemented by spouts is that they read a message from a messaging queue, say RabbitMQ, produce the tuple into the topology for further processing, and then dequeue the message once it receives the acknowledgement that the tuple has been processed completely. When one of the bolts in the topology needs to produce a new tuple in the course of processing a message, for example, bolt B in the preceding topology, then it should emit the new tuple anchored with the original tuple that it got from the spout. Whenever you are done with processing a tuple in the execute method of your bolt, send an acknowledgment. If there is some problem in processing a tuple, a failure signal should be sent back. One of the general patterns of processing in Storm bolts is to process a tuple in, emit new tuples, and send an acknowledgement at the end of the execute method . This model results in at-least-once message processing semantics, and your application should be ready to handle a scenario when some of the messages will be processed multiple times .","title":"At-least-one Message Processing"},{"location":"notes/module-01/03-parallelism-and-partitioning/#tick-tuple","text":"The tick tuple is a storm-generated tuple that we can configure at each bolt level, mainly for executing periodic tasks in the bolts (like cleaning a cache). The developer can configure the tick tuple at the code level while writing a bolt. public Map < String , Object > getComponentConfiguration () { Config conf = new Config (); int tickFrequencyInSeconds = 10 ; conf . put ( Config . TOPOLOGY_TICK_TUPLE_FREQ_SECS , tickFrequencyInSeconds ); return conf ; } In the preceding code, we have configured the tick tuple time to 10 seconds. Now, Storm will start generating a tick tuple after every 10 seconds. We'd require to prepare our code to identify it: public void execute ( Tuple tuple ) { if ( isTickTuple ( tuple )) { // now you can trigger e.g. a periodic activity } else { // do something with the normal tuple } } private static boolean isTickTuple ( Tuple tuple ) { return tuple . getSourceComponent (). equals ( Constants . SYSTEM_COMPONENT_ID ) && tuple . getSourceStreamId (). equals ( Constants . SYSTEM_TICK_STREAM_ID ); }","title":"Tick tuple"},{"location":"notes/module-01/code-sample-instructions/","text":"","title":"Code sample instructions"}]}